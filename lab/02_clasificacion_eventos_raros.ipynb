{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c935f8f0",
   "metadata": {},
   "source": [
    "# Laboratorio 2 — Clasificación: detección de eventos raros (dataset sintético)\n",
    "\n",
    "Aprenderás a diseñar un pipeline de clasificación y a evaluar correctamente cuando hay desbalance de clases.\n",
    "\n",
    "## Objetivos\n",
    "- Generar o cargar datos y entender el desbalance.\n",
    "- Entrenar un baseline lineal y un modelo de ensamble.\n",
    "- Usar métricas adecuadas (F1, matriz de confusión, ROC-AUC) y umbrales.\n",
    "\n",
    "## Requisitos\n",
    "- Laboratorio 0\n",
    "- scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4d987",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "Muchos problemas de IA en industria son *detección de eventos raros*: fraude, fallos, intrusiones, etc.\n",
    "En estos escenarios **Accuracy** puede engañar: si el 98% es clase negativa, un modelo que siempre diga “no” ya logra 98%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a1c7e",
   "metadata": {},
   "source": [
    "## 1) Crear un dataset con desbalance\n",
    "\n",
    "Usaremos `make_classification` para simular datos tabulares sin depender de descargas externas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=6000,\n",
    "    n_features=20,\n",
    "    n_informative=6,\n",
    "    n_redundant=2,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.97, 0.03],  # 3% positivos\n",
    "    flip_y=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f\"f{i:02d}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y, name=\"evento_raro\")\n",
    "\n",
    "y.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9479a2",
   "metadata": {},
   "source": [
    "## 2) Split + baseline\n",
    "\n",
    "Empezamos con regresión logística (con escalado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d244bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=500, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a3933",
   "metadata": {},
   "source": [
    "## 3) Métricas que importan\n",
    "\n",
    "Además de F1, observaremos:\n",
    "- Matriz de confusión\n",
    "- Curva ROC y AUC\n",
    "- Precisión/Recall\n",
    "\n",
    "La pregunta clave: **¿prefieres falsos positivos o falsos negativos?** Depende del negocio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "y_proba = pipe_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991195f6",
   "metadata": {},
   "source": [
    "## 4) Modelo alternativo: Gradient Boosting\n",
    "\n",
    "Los modelos de boosting suelen rendir muy bien en tabular. Aquí probamos `HistGradientBoostingClassifier`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "gb = HistGradientBoostingClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.08,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "y_proba_gb = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred_gb, digits=3))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ece13",
   "metadata": {},
   "source": [
    "## 5) Ajuste de umbral\n",
    "\n",
    "Por defecto, muchos clasificadores usan umbral 0.5. Con clases desbalanceadas, conviene seleccionar el umbral por una métrica objetivo (p.ej., maximizar F1 o asegurar recall mínimo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "rows = []\n",
    "for t in thresholds:\n",
    "    pred_t = (y_proba_gb >= t).astype(int)\n",
    "    rows.append({\n",
    "        \"umbral\": t,\n",
    "        \"precision\": precision_score(y_test, pred_t, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, pred_t),\n",
    "        \"f1\": f1_score(y_test, pred_t, zero_division=0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c490e",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "### Ejercicio 1: Métrica orientada a negocio\n",
    "Define un coste para FP y FN (por ejemplo, FP=1, FN=10). Encuentra el umbral que minimiza el coste esperado.\n",
    "\n",
    "**Entregables**\n",
    "- Función de coste\n",
    "- Tabla coste vs umbral\n",
    "- Umbral óptimo y justificación\n",
    "\n",
    "**Criterios de evaluación**\n",
    "- El coste se calcula correctamente\n",
    "- Se justifica el umbral con números\n",
    "- Se discute el trade-off\n",
    "\n",
    "### Ejercicio 2: Comparación de modelos\n",
    "Añade un tercer modelo (p.ej., `RandomForestClassifier` o `SVC`) y compáralo con LR y GB usando las mismas métricas.\n",
    "\n",
    "**Entregables**\n",
    "- Tabla comparativa\n",
    "- Conclusión\n",
    "\n",
    "**Pistas**\n",
    "- En desbalance, prueba `class_weight='balanced'` o muestreo si procede\n",
    "\n",
    "**Criterios de evaluación**\n",
    "- Comparación justa (mismo split)\n",
    "- Métricas relevantes\n",
    "- Conclusión argumentada\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
